{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6306077",
   "metadata": {},
   "source": [
    "# Standard Data Wrangling Commands\n",
    "\n",
    "***Raku-centric, version 0.9***   \n",
    "Anton Antonov   \n",
    "[RakuForPrediction-book at GitHub](https://github.com/antononcube/RakuForPrediction-book)   \n",
    "[SimplifiedMachineLearningWorkflows-book at GitHub](https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book)   \n",
    "September 2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This document demonstrates and exemplifies the abilities of the package\n",
    "[\"DSL::English::DataQueryWorkflow\"](https://raku.land/zef:antononcube/DSL::English::DataQueryWorkflows), [AAp1],\n",
    "to produce executable code that fits majority of the data wrangling use cases.\n",
    "\n",
    "The examples should give a good idea of the English-based Domain Specific Language (DSL)\n",
    "utilized by [AAp1].\n",
    "\n",
    "The data wrangling in Raku is done with the packages:\n",
    "[\"Data::Generators\"](https://raku.land/zef:antononcube/Data::Generators),\n",
    "[\"Data::Reshapers\"](https://raku.land/zef:antononcube/Data::Reshapers), and\n",
    "[\"Data::Summarizers\"](https://raku.land/zef:antononcube/Data::Summarizers).\n",
    "\n",
    "This document has examples that were used in the presentation [“Multi-language Data-Wrangling Conversational Agent”](https://www.youtube.com/watch?v=pQk5jwoMSxs), [AAv1]. \n",
    "That presentation is an introduction to data wrangling from a more general, multi-language perspective.\n",
    "It is assumed that the readers of this document are familiar with the general data processing workflow discussed in the presentation [AAv1]. Additional examples are discussed in the presentation [\"Implementation of ML algorithms in Raku\"](https://www.youtube.com/watch?v=efRHfjYebs4), [AAv2], (given at [TRC-2022](https://conf.raku.org/talk/170).)\n",
    "\n",
    "For detailed introduction into data wrangling (with- and in Raku) see the article\n",
    "[\"Introduction to data wrangling with Raku\"](https://rakuforprediction.wordpress.com/2021/12/31/introduction-to-data-wrangling-with-raku/),\n",
    "[AA1]. (And its Bulgarian version [AA2].)\n",
    "\n",
    "Some of the data is acquired with the package\n",
    "[\"Data::ExampleDatasets\"](https://raku.land/zef:antononcube/Data::ExampleDatasets).\n",
    "\n",
    "The data wrangling sections have two parts: a code generation part, and an execution steps part. \n",
    "\n",
    "### Document execution\n",
    "\n",
    "This document has a Jupyter notebook version and a Markdownd file version. (\"Document\" is often used to mean any of those versions or both versions.)\n",
    "\n",
    "The Markdown file version is a \"computable Markdown document\" -- the Raku cells are (context-consecutively) evaluated with the\n",
    "[\"literate programming\"](https://en.wikipedia.org/wiki/Literate_programming)\n",
    "package\n",
    "[\"Text::CodeProcessing\"](https://raku.land/cpan:ANTONOV/Text::CodeProcessing), [AA3, AAp7].\n",
    "\n",
    "### Other programming languages\n",
    "\n",
    "Versions of this document (or notebook) are made for other programming languages: Python, R, and Wolfram Language (WL). Here is a list of all notebooks hosted at GitHub repository [\"RakuForPrediction-book\"](https://github.com/antononcube/RakuForPrediction-book):\n",
    "\n",
    "- [Python](https://github.com/antononcube/RakuForPrediction-book/blob/main/Notebooks/Jupyter/Standard-data-wrangling-commands-Python.ipynb)\n",
    "\n",
    "- [R](https://github.com/antononcube/RakuForPrediction-book/blob/main/Notebooks/Jupyter/Standard-data-wrangling-commands-R.ipynb)\n",
    "\n",
    "- [Raku](https://github.com/antononcube/RakuForPrediction-book/blob/main/Notebooks/Jupyter/Standard-data-wrangling-commands-Raku.ipynb)\n",
    "\n",
    "- [WL](https://github.com/antononcube/RakuForPrediction-book/blob/main/Notebooks/Jupyter/Standard-data-wrangling-commands-WL.ipynb)\n",
    "\n",
    "**Remark:** Wolfram Language (WL) and Mathematica are used as synonyms.\n",
    "\n",
    "------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d43ef",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7a5573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Raku::Reshapers"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $examplesTarget = 'Raku::Reshapers';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac687914",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6c35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "use Stats;\n",
    "use Data::ExampleDatasets;\n",
    "use Data::Reshapers;\n",
    "use Data::Summarizers;\n",
    "use Text::Plot;\n",
    "\n",
    "use DSL::English::DataQueryWorkflows;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bfedf",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "#### Titanic data\n",
    "\n",
    "We can obtain the Titanic dataset using the function `get-titanic-dataset` provided by \"Data::Reshapers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12794e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my @dfTitanic = get-titanic-dataset();\n",
    "dimensions(@dfTitanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f202337",
   "metadata": {},
   "source": [
    "#### Anscombe's quartet\n",
    "\n",
    "The dataset named\n",
    "[\"Anscombe's quartet\"](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
    "has four datasets that have nearly identical simple descriptive statistics, \n",
    "yet have very different distributions and appear very different when graphed.\n",
    "\n",
    "Anscombe's quartet is (usually) given in a table with eight columns that is somewhat awkward to work with.\n",
    "Below we demonstrate data transformations that make plotting of the four datasets easier.\n",
    "The DSL specifications used make those data transformations are programming language independent.\n",
    "\n",
    "We can obtain the Anscombe's dataset using the function `example-dataset` provided by [\"Data::ExampleDatasets\"](https://raku.land/zef:antononcube/Data::ExampleDatasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b8845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+----------------+-----------------+----------------+\n",
      "| passengerSurvival | passengerSex  | passengerAge   | id              | passengerClass |\n",
      "+-------------------+---------------+----------------+-----------------+----------------+\n",
      "| died     => 809   | male   => 843 | 20      => 334 | 20      => 1    | 3rd => 709     |\n",
      "| survived => 500   | female => 466 | -1      => 263 | 201     => 1    | 1st => 323     |\n",
      "|                   |               | 30      => 258 | 717     => 1    | 2nd => 277     |\n",
      "|                   |               | 40      => 190 | 691     => 1    |                |\n",
      "|                   |               | 50      => 88  | 838     => 1    |                |\n",
      "|                   |               | 60      => 57  | 804     => 1    |                |\n",
      "|                   |               | 0       => 56  | 947     => 1    |                |\n",
      "|                   |               | (Other) => 63  | (Other) => 1302 |                |\n",
      "+-------------------+---------------+----------------+-----------------+----------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({id => 20      => 1, passengerAge => 20      => 334, passengerClass => 3rd => 709, passengerSex => male   => 843, passengerSurvival => died     => 809} {id => 201     => 1, passengerAge => -1      => 263, passengerClass => 1st => 323, passengerSex => female => 466, passengerSurvival => survived => 500} {id => 717     => 1, passengerAge => 30      => 258, passengerClass => 2nd => 277, passengerSex => , passengerSurvival => } {id => 691     => 1, passengerAge => 40      => 190, passengerClass => , passengerSex => , passengerSurvival => } {id => 838     => 1, passengerAge => 50      => 88, passengerClass => , passengerSex => , passengerSurvival => } {id => 804     => 1, passengerAge => 60      => 57, passengerClass => , passengerSex => , passengerSurvival => } {id => 947     => 1, passengerAge => 0       => 56, passengerClass => , passengerSex => , passengerSurvival => } {id => (Other) => 1302, passengerAge => (Other) => 63, passengerClass => , passengerSex => , passengerSurvival => })"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records-summary(@dfTitanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04a4b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my @dfAnscombe = |example-dataset('anscombe');\n",
    "@dfAnscombe = |@dfAnscombe.map({ %( $_.keys Z=> $_.values>>.Numeric) });\n",
    "dimensions(@dfAnscombe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9806d",
   "metadata": {},
   "source": [
    "#### Star Wars films data\n",
    "\n",
    "We can obtain\n",
    "[Star Wars films](https://en.wikipedia.org/wiki/List_of_Star_Wars_films)\n",
    "datasets using (again) the function `example-dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bde78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dfStarwars => (87 11)\n",
      "@dfStarwarsFilms => (173 2)\n",
      "@dfStarwarsStarships => (31 2)\n",
      "@dfStarwarsVehicles => (13 2)\n"
     ]
    }
   ],
   "source": [
    "my @dfStarwars = example-dataset(\"https://raw.githubusercontent.com/antononcube/R-packages/master/DataQueryWorkflowsTests/inst/extdata/dfStarwars.csv\");\n",
    "my @dfStarwarsFilms = example-dataset(\"https://raw.githubusercontent.com/antononcube/R-packages/master/DataQueryWorkflowsTests/inst/extdata/dfStarwarsFilms.csv\");\n",
    "my @dfStarwarsStarships = example-dataset(\"https://raw.githubusercontent.com/antononcube/R-packages/master/DataQueryWorkflowsTests/inst/extdata/dfStarwarsStarships.csv\");\n",
    "my @dfStarwarsVehicles = example-dataset(\"https://raw.githubusercontent.com/antononcube/R-packages/master/DataQueryWorkflowsTests/inst/extdata/dfStarwarsVehicles.csv\");\n",
    "\n",
    ".say for <@dfStarwars @dfStarwarsFilms @dfStarwarsStarships @dfStarwarsVehicles>.map({ $_ => dimensions(::($_)) })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe23de1",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Multi-language translation\n",
    "\n",
    "In this section show that the Raku package [“DSL::English::DataQueryWorkflows”](https://raku.land/zef:antononcube/DSL::English::DataQueryWorkflows) generates code for multiple programming languages. \n",
    "Also, it translates the English DSL into DSLs of other natural languages.\n",
    "\n",
    "### Programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34aab731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python:\n",
      "obj = dfTitanic.copy()\n",
      "obj = obj.groupby([\"passengerClass\"])\n",
      "obj = obj.size()\n",
      "\n",
      "Raku:\n",
      "$obj = dfTitanic ;\n",
      "$obj = group-by($obj, \"passengerClass\") ;\n",
      "$obj = $obj>>.elems\n",
      "\n",
      "R:\n",
      "obj <- dfTitanic ;\n",
      "obj <- split( x = obj, f = obj[[\"passengerClass\"]] ) ;\n",
      "obj = length(obj)\n",
      "\n",
      "R::tidyverse:\n",
      "dfTitanic %>%\n",
      "dplyr::group_by(passengerClass) %>%\n",
      "dplyr::count()\n",
      "\n",
      "WL:\n",
      "obj = dfTitanic;\n",
      "obj = GroupBy[ obj, #[\"passengerClass\"]& ];\n",
      "obj = Map[ Length, obj]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True True True True True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command0 = 'use dfTitanic; group by passengerClass; counts;';\n",
    "<Python Raku R R::tidyverse WL>.map({ say \"\\n{ $_ }:\\n\", ToDataQueryWorkflowCode($command0, target => $_) });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a37218",
   "metadata": {},
   "source": [
    "### Natural languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3070d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulgarian:\n",
      "използвай таблицата: dfTitanic\n",
      "групирай с колоните: passengerClass\n",
      "намери броя\n",
      "\n",
      "Korean:\n",
      "테이블 사용: dfTitanic\n",
      "열로 그룹화: passengerClass\n",
      "하위 그룹의 크기 찾기\n",
      "\n",
      "Russian:\n",
      "использовать таблицу: dfTitanic\n",
      "групировать с колонками: passengerClass\n",
      "найти число\n",
      "\n",
      "Spanish:\n",
      "utilizar la tabla: dfTitanic\n",
      "agrupar con columnas: \"passengerClass\"\n",
      "encontrar recuentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True True True True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<Bulgarian Korean Russian Spanish>.map({ say \"\\n{ $_ }:\\n\", ToDataQueryWorkflowCode($command0, target => $_) });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7a138",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Using DSL cells\n",
    "\n",
    "If the package [\"DSL::Shared::Utilities::ComprehensiveTranslations\"](https://github.com/antononcube/Raku-DSL-Shared-Utilities-ComprehensiveTranslation), [AAp3], is installed\n",
    "then DSL specifications can be directly written in the Markdown cells.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```raku-dsl\n",
    "DSL TARGET Python::pandas;\n",
    "include setup code;\n",
    "use dfStarwars;\n",
    "join with dfStarwarsFilms by \"name\"; \n",
    "group by species; \n",
    "counts;\n",
    "```\n",
    "\n",
    "------\n",
    "\n",
    "## Trivial workflow\n",
    "\n",
    "In this section we demonstrate code generation and execution results for very simple (and very frequently used) sequence of data wrangling operations.\n",
    "\n",
    "### Code generation\n",
    "\n",
    "For the simple specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93e63be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use dfTitanic; group by passengerClass; counts;\n"
     ]
    }
   ],
   "source": [
    "say $command0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d40bc",
   "metadata": {},
   "source": [
    "We generate target code with `ToDataQueryWorkflowCode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b4c53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfTitanic ;\n",
       "$obj = group-by($obj, \"passengerClass\") ;\n",
       "$obj = $obj>>.elems"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToDataQueryWorkflowCode($command0, target => $examplesTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be229bc",
   "metadata": {},
   "source": [
    "### Execution steps\n",
    "\n",
    "Get the dataset into a \"pipeline object\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df481f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $obj = @dfTitanic;\n",
    "dimensions($obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4cabe3",
   "metadata": {},
   "source": [
    "Group by column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e894de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = group-by($obj, \"passengerClass\") ;\n",
    "$obj.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e5eb6",
   "metadata": {},
   "source": [
    "Assign group sizes to the \"pipeline object\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f7ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1st => 323, 2nd => 277, 3rd => 709}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = $obj>>.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db237cac",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Cross tabulation\n",
    "\n",
    "[Cross tabulation](https://en.wikipedia.org/wiki/Contingency_table) \n",
    "is a fundamental data wrangling operation. For the related transformations to long- and wide-format\n",
    "see the section \"Complicated and neat workflow\".\n",
    "\n",
    "### Code generation\n",
    "\n",
    "Here we define a command that filters the Titanic dataset and then makes cross-tabulates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d45acd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfTitanic ;\n",
       "$obj = $obj.grep({ $_{\"passengerSex\"} eq \"male\" and $_{\"passengerSurvival\"} eq \"died\" or $_{\"passengerSurvival\"} eq \"survived\" }).Array ;\n",
       "$obj = cross-tabulate( $obj, \"passengerClass\", \"passengerSurvival\", \"passengerAge\" )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command1 = \"use dfTitanic;\n",
    "filter with passengerSex is 'male' and passengerSurvival equals 'died' or passengerSurvival is 'survived' ;\n",
    "cross tabulate passengerClass, passengerSurvival over passengerAge;\";\n",
    "\n",
    "ToDataQueryWorkflowCode($command1, target => $examplesTarget);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba15d29",
   "metadata": {},
   "source": [
    "### Execution steps\n",
    "\n",
    "Copy the Titanic data into a \"pipeline object\" and show its dimensions and a sample of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a3b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dimensions:(1309 5)\n",
      "+-------------------+------+--------------+--------------+----------------+\n",
      "| passengerSurvival |  id  | passengerSex | passengerAge | passengerClass |\n",
      "+-------------------+------+--------------+--------------+----------------+\n",
      "|      survived     | 551  |    female    |      20      |      2nd       |\n",
      "|        died       | 1231 |    female    |      0       |      3rd       |\n",
      "|      survived     | 135  |    female    |      -1      |      1st       |\n",
      "|      survived     | 1255 |     male     |      20      |      3rd       |\n",
      "|      survived     | 895  |     male     |      0       |      3rd       |\n",
      "|      survived     | 260  |     male     |      30      |      1st       |\n",
      "|        died       | 229  |     male     |      20      |      1st       |\n",
      "+-------------------+------+--------------+--------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "my $obj = @dfTitanic ;\n",
    "say \"Titanic dimensions:\", dimensions(@dfTitanic);\n",
    "say to-pretty-table($obj.pick(7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9566eb",
   "metadata": {},
   "source": [
    "Filter the data and show the number of rows in the result set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1427664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n"
     ]
    }
   ],
   "source": [
    "$obj = $obj.grep({ $_{\"passengerSex\"} eq \"male\" and $_{\"passengerSurvival\"} eq \"died\" or $_{\"passengerSurvival\"} eq \"survived\" }).Array ;\n",
    "say $obj.elems;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14eb4e",
   "metadata": {},
   "source": [
    "Cross tabulate and show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d5ff850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n",
      "|     | survived | died |\n",
      "+-----+----------+------+\n",
      "| 1st |   6671   | 4290 |\n",
      "| 2nd |   2776   | 4419 |\n",
      "| 3rd |   2720   | 7562 |\n",
      "+-----+----------+------+\n"
     ]
    }
   ],
   "source": [
    "$obj = cross-tabulate( $obj, \"passengerClass\", \"passengerSurvival\", \"passengerAge\" );\n",
    "say to-pretty-table($obj);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b52d35",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Mutation with formulas\n",
    "\n",
    "In this section we discuss formula utilization to mutate data. We show how to use column references.\n",
    "\n",
    "Special care has to be taken when the specifying data mutations with formulas that reference to columns in the dataset.\n",
    "\n",
    "The code corresponding to the `transform ...` line in this example produces \n",
    "*expected* result for the target \"R::tidyverse\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a607399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfStarwars %>%\n",
       "dplyr::select(name, homeworld, mass, height) %>%\n",
       "dplyr::mutate(bmi = mass/height^2*10000) %>%\n",
       "dplyr::filter(bmi >= 30 & height < 200) %>%\n",
       "dplyr::arrange(desc(mass), desc(height))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command2 = \"use data frame dfStarwars;\n",
    "keep the columns name, homeworld, mass & height;\n",
    "transform with bmi = `mass/height^2*10000`;\n",
    "filter rows by bmi >= 30 & height < 200;\n",
    "arrange by the variables mass & height descending\";\n",
    "\n",
    "ToDataQueryWorkflowCode($command2, target => 'R::tidyverse');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e78a7",
   "metadata": {},
   "source": [
    "Specifically, for [\"Raku::Reshapers\"](https://raku.land/zef:antononcube/Data::Reshapers) the transform specification line has to refer to the context variable `$_`.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33e9890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfStarwars ;\n",
       "$obj = $obj.map({ $_{\"bmi\"} = $_<mass>/$_<height>^2*10000; $_{\"homeworld\"} = $_<homeworld>.uc; $_ })"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command2r = 'use data frame dfStarwars;\n",
    "transform with bmi = `$_<mass>/$_<height>^2*10000` and homeworld = `$_<homeworld>.uc`;';\n",
    "\n",
    "ToDataQueryWorkflowCode($command2r, target => 'Raku::Reshapers');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c8f56",
   "metadata": {},
   "source": [
    "**Remark:** Note that we have to use single quotes for the command assignment; \n",
    "using double quotes will invoke Raku's string interpolation feature. \n",
    "\n",
    "------\n",
    "\n",
    "## Grouping awareness\n",
    "\n",
    "In this section we discuss the treatment of multiple \"group by\" invocations within the same DSL specification.\n",
    "\n",
    "### Code generation \n",
    "\n",
    "Since there is no expectation to have a dedicated data transformation monad -- in whatever programming language -- we\n",
    "can try to make the command sequence parsing to be \"aware\" of the grouping operations.\n",
    "\n",
    "In the following example before applying the grouping operation in fourth line \n",
    "we have to flatten the data (which is grouped in the second line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2135b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfTitanic ;\n",
       "$obj = group-by($obj, \"passengerClass\") ;\n",
       "say \"counts: \", $obj>>.elems ;\n",
       "$obj = group-by($obj.values.reduce( -> $x, $y { [|$x, |$y] } ), \"passengerSex\") ;\n",
       "$obj = $obj>>.elems"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command3 = \"use dfTitanic; \n",
    "group by passengerClass; \n",
    "echo counts; \n",
    "group by passengerSex; \n",
    "counts\";\n",
    "\n",
    "ToDataQueryWorkflowCode($command3, target => $examplesTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e9004",
   "metadata": {},
   "source": [
    "### Execution steps\n",
    "\n",
    "First grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74000a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: {1st => 323, 2nd => 277, 3rd => 709}\n"
     ]
    }
   ],
   "source": [
    "my $obj = @dfTitanic ;\n",
    "$obj = group-by($obj, \"passengerClass\") ;\n",
    "say \"counts: \", $obj>>.elems ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f1bfc",
   "metadata": {},
   "source": [
    "Before doing the second grouping we flatten the groups of the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "073fb16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{female => 466, male => 843}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = group-by($obj.values.reduce( -> $x, $y { [|$x, |$y] } ), \"passengerSex\") ;\n",
    "$obj = $obj>>.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b947eed",
   "metadata": {},
   "source": [
    "Instead of `reduce` we can use the function `flatten` (provided by \"Data::Reshapers\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cc3ba2d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: {1st => 323, 2nd => 277, 3rd => 709}\n",
      "counts: {female => 466, male => 843}\n"
     ]
    }
   ],
   "source": [
    "my $obj2 = group-by(@dfTitanic , \"passengerClass\") ;\n",
    "say \"counts: \", $obj2>>.elems ;\n",
    "$obj2 = group-by(flatten($obj2.values, max-level => 1).Array, \"passengerSex\") ;\n",
    "say \"counts: \", $obj2>>.elems;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a274691",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Non-trivial workflow\n",
    "\n",
    "In this section we generate and demonstrate data wrangling steps that clean, mutate, filter, group, and summarize a given dataset.\n",
    "\n",
    "### Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "607c638d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfStarwars ;\n",
       "$obj = $obj.deepmap({ ( ($_ eqv Any) or $_.isa(Nil) or $_.isa(Whatever) ) ?? <NaN> !! $_ }) ;\n",
       "$obj = $obj.map({ $_{\"mass\"} = +$_<mass>; $_{\"height\"} = +$_<height>; $_ }) ;\n",
       "say \"dimensions: {dimensions($obj)}\" ;\n",
       "records-summary($obj) ;\n",
       "$obj = $obj.grep({ $_{\"birth_year\"} > 27 }).Array ;\n",
       "$obj = select-columns($obj, (\"homeworld\", \"mass\", \"height\") ) ;\n",
       "$obj = group-by($obj, \"homeworld\") ;\n",
       "say \"counts: \", $obj>>.elems ;\n",
       "$obj = $obj.map({ $_.key => summarize-at($_.value, (\"mass\", \"height\"), (&mean, &median)) })"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command4 = '\n",
    "use dfStarwars;\n",
    "replace missing with `<NaN>`;\n",
    "mutate with mass = `+$_<mass>` and height = `+$_<height>`;\n",
    "show dimensions;\n",
    "echo summary;\n",
    "filter by birth_year greater than 27;\n",
    "select homeworld, mass and height;\n",
    "group by homeworld;\n",
    "show counts;\n",
    "summarize the variables mass and height with &mean and &median\n",
    "';\n",
    "\n",
    "ToDataQueryWorkflowCode($command4, target => $examplesTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a24021",
   "metadata": {},
   "source": [
    "### Execution steps\n",
    "\n",
    "Here is code that cleans the data of missing values, and shows dimensions and summary (corresponds to the first five lines above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04e17d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:\n",
      "+-----------------------------------------+------------------+----------------+----------------------------------------+-----------------+----------------------------------------+---------------+----------------------+-----------------+---------------+---------------+\n",
      "| height                                  | name             | species        | mass                                   | homeworld       | birth_year                             | hair_color    | sex                  | gender          | eye_color     | skin_color    |\n",
      "+-----------------------------------------+------------------+----------------+----------------------------------------+-----------------+----------------------------------------+---------------+----------------------+-----------------+---------------+---------------+\n",
      "| Min                       => 66         | Mon Mothma => 1  | Human    => 35 | Min                       => 15        | Naboo     => 11 | Min                       => 8         | none    => 37 | male           => 60 | masculine => 66 | brown   => 21 | fair    => 17 |\n",
      "| 1st-Qu                    => 166.5      | Eeth Koth  => 1  | Droid    => 6  | 1st-Qu                    => 55        | Tatooine  => 10 | 1st-Qu                    => 33        | brown   => 18 | female         => 16 | feminine  => 17 | blue    => 19 | light   => 11 |\n",
      "| Mean                      => 174.358025 | Kit Fisto  => 1  | NaN      => 4  | Mean                      => 97.311864 | NaN       => 10 | Mean                      => 87.565116 | black   => 13 | none           => 6  | NaN       => 4  | yellow  => 11 | dark    => 6  |\n",
      "| Median                    => 180        | Mace Windu => 1  | Gungan   => 3  | Median                    => 79        | Coruscant => 3  | Median                    => 52        | NaN     => 5  | NaN            => 4  |                 | black   => 10 | grey    => 6  |\n",
      "| 3rd-Qu                    => 191        | Dooku      => 1  | Twi'lek  => 2  | 3rd-Qu                    => 85        | Kamino    => 3  | 3rd-Qu                    => 72        | white   => 4  | hermaphroditic => 1  |                 | orange  => 8  | green   => 6  |\n",
      "| Max                       => 264        | Adi Gallia => 1  | Mirialan => 2  | Max                       => 1358      | Alderaan  => 3  | Max                       => 896       | blond   => 3  |                      |                 | red     => 5  | pale    => 5  |\n",
      "| (Any-Nan-Nil-or-Whatever) => 6          | Zam Wesell => 1  | Kaminoan => 2  | (Any-Nan-Nil-or-Whatever) => 28        | Corellia  => 2  | (Any-Nan-Nil-or-Whatever) => 44        | grey    => 1  |                      |                 | hazel   => 3  | brown   => 4  |\n",
      "|                                         | (Other)    => 80 | (Other)  => 33 |                                        | (Other)   => 45 |                                        | (Other) => 6  |                      |                 | (Other) => 10 | (Other) => 32 |\n",
      "+-----------------------------------------+------------------+----------------+----------------------------------------+-----------------+----------------------------------------+---------------+----------------------+-----------------+---------------+---------------+\n",
      "dimensions: 87 11\n"
     ]
    }
   ],
   "source": [
    "my $obj = @dfStarwars ;\n",
    "$obj = $obj.deepmap({ ( ($_ eqv Any) or $_.isa(Nil) or $_.isa(Whatever) ) ?? <NaN> !! $_ }) ;\n",
    "$obj = $obj.map({ $_{\"mass\"} = +$_<mass>; $_{\"height\"} = +$_<height>; $_ }).Array ;\n",
    "say \"summary:\" ;\n",
    "records-summary($obj);\n",
    "say \"dimensions: {dimensions($obj)}\" ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b8c2d",
   "metadata": {},
   "source": [
    "Here is the deduced type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f0daa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector((Any), 87)\n"
     ]
    }
   ],
   "source": [
    "say deduce-type($obj);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd222a0",
   "metadata": {},
   "source": [
    "Here is a sample of the dataset (wrangled so far):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b98a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-----------+--------+--------------+--------------+-----------+-----------+------+------------+--------+\n",
      "|        name        |  skin_color  | eye_color |  sex   |  hair_color  |   species    | homeworld |   gender  | mass | birth_year | height |\n",
      "+--------------------+--------------+-----------+--------+--------------+--------------+-----------+-----------+------+------------+--------+\n",
      "|     Wat Tambor     | green, grey  |  unknown  |  male  |     none     |   Skakoan    |   Skako   | masculine |  48  |    NaN     |  193   |\n",
      "|       Bossk        |    green     |    red    |  male  |     none     |  Trandoshan  | Trandosha | masculine | 113  |     53     |  190   |\n",
      "|       Ackbar       | brown mottle |   orange  |  male  |     none     | Mon Calamari |  Mon Cala | masculine |  83  |     41     |  180   |\n",
      "|   Wilhuff Tarkin   |     fair     |    blue   |  male  | auburn, grey |    Human     |   Eriadu  | masculine | NaN  |     64     |  180   |\n",
      "| Beru Whitesun lars |    light     |    blue   | female |    brown     |    Human     |  Tatooine |  feminine |  75  |     47     |  165   |\n",
      "|    Arvel Crynyd    |     fair     |   brown   |  male  |    brown     |    Human     |    NaN    | masculine | NaN  |    NaN     |  NaN   |\n",
      "|        Finn        |     dark     |    dark   |  male  |    black     |    Human     |    NaN    | masculine | NaN  |    NaN     |  NaN   |\n",
      "+--------------------+--------------+-----------+--------+--------------+--------------+-----------+-----------+------+------------+--------+\n"
     ]
    }
   ],
   "source": [
    "say to-pretty-table($obj.pick(7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f8952",
   "metadata": {},
   "source": [
    "Here we group by \"homeworld\" and show counts for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca1a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: {Alderaan => 3, Aleen Minor => 1, Bespin => 1, Bestine IV => 1, Cato Neimoidia => 1, Cerea => 1, Champala => 1, Chandrila => 1, Concord Dawn => 1, Corellia => 2, Coruscant => 3, Dathomir => 1, Dorin => 1, Endor => 1, Eriadu => 1, Geonosis => 1, Glee Anselm => 1, Haruun Kal => 1, Iktotch => 1, Iridonia => 1, Kalee => 1, Kamino => 3, Kashyyyk => 2, Malastare => 1, Mirial => 2, Mon Cala => 1, Muunilinst => 1, NaN => 10, Naboo => 11, Nal Hutta => 1, Ojom => 1, Quermia => 1, Rodia => 1, Ryloth => 2, Serenno => 1, Shili => 1, Skako => 1, Socorro => 1, Stewjon => 1, Sullust => 1, Tatooine => 10, Toydaria => 1, Trandosha => 1, Troiken => 1, Tund => 1, Umbara => 1, Utapau => 1, Vulpter => 1, Zolan => 1}\n"
     ]
    }
   ],
   "source": [
    "$obj = group-by($obj, \"homeworld\") ;\n",
    "say \"counts: \", $obj>>.elems ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26c3bd",
   "metadata": {},
   "source": [
    "Here is summarization at specified columns with specified functions (from the \"Stats\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a36859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+-------------+---------------+\n",
      "|             | mass.mean  | height.mean | mass.median | height.median |\n",
      "+-------------+------------+-------------+-------------+---------------+\n",
      "| Corellia    | 78.500000  |  175.000000 |  78.500000  |   175.000000  |\n",
      "| Coruscant   |    NaN     |  173.666667 |     NaN     |   170.000000  |\n",
      "| Glee Anselm | 87.000000  |  196.000000 |  87.000000  |   196.000000  |\n",
      "| Kashyyyk    | 124.000000 |  231.000000 |  124.000000 |   231.000000  |\n",
      "| Mirial      | 53.100000  |  168.000000 |  53.100000  |   168.000000  |\n",
      "| Socorro     | 79.000000  |  177.000000 |  79.000000  |   177.000000  |\n",
      "| Umbara      | 48.000000  |  178.000000 |  48.000000  |   178.000000  |\n",
      "+-------------+------------+-------------+-------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "$obj = $obj.map({ $_.key => summarize-at($_.value, (\"mass\", \"height\"), (&mean, &median)) });\n",
    "say to-pretty-table($obj.pick(7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217445a",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Joins\n",
    "\n",
    "In this section we demonstrate the fundamental operation of joining two datasets.\n",
    "\n",
    "### Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eab1aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfStarwarsFilms ;\n",
       "$obj = join-across( $obj, dfStarwars, (\"name\"), join-spec=>\"Left\") ;\n",
       "$obj = $obj.deepmap({ ( ($_ eqv Any) or $_.isa(Nil) or $_.isa(Whatever) ) ?? <NaN> !! $_ }) ;\n",
       "$obj = $obj.sort({($_{\"name\"}, $_{\"film\"}) }).reverse.Array ;\n",
       "$obj"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command5 = \"use dfStarwarsFilms;\n",
    "left join with dfStarwars by 'name';\n",
    "replace missing with `<NaN>`;\n",
    "sort by name, film desc;\n",
    "take pipeline value\";\n",
    "\n",
    "ToDataQueryWorkflowCode($command5, target => $examplesTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25545",
   "metadata": {},
   "source": [
    "### Execution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "408957ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------------------+----------------+-----------------------+\n",
       "|         film         |    species     |          name         |\n",
       "+----------------------+----------------+-----------------------+\n",
       "| Attack of the Clones |    Clawdite    |       Zam Wesell      |\n",
       "|  Return of the Jedi  | Yoda's species |          Yoda         |\n",
       "|  The Phantom Menace  |    Quermian    |      Yarael Poof      |\n",
       "|      A New Hope      |     Human      |     Wilhuff Tarkin    |\n",
       "|  Return of the Jedi  |      Ewok      | Wicket Systri Warrick |\n",
       "|      A New Hope      |     Human      |     Wedge Antilles    |\n",
       "|  The Phantom Menace  |   Toydarian    |         Watto         |\n",
       "| Attack of the Clones |    Skakoan     |       Wat Tambor      |\n",
       "| Revenge of the Sith  |     Pau'an     |       Tion Medon      |\n",
       "| Attack of the Clones |    Kaminoan    |        Taun We        |\n",
       "| Revenge of the Sith  |    Wookiee     |        Tarfful        |\n",
       "| Revenge of the Sith  |      NaN       |       Sly Moore       |\n",
       "+----------------------+----------------+-----------------------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = @dfStarwarsFilms ;\n",
    "$obj = join-across( $obj, select-columns( @dfStarwars, <name species>), (\"name\"), join-spec=>\"Left\") ;\n",
    "$obj = $obj.deepmap({ ( ($_ eqv Any) or $_.isa(Nil) or $_.isa(Whatever) ) ?? <NaN> !! $_ }) ;\n",
    "$obj = $obj.sort({($_{\"name\"}, $_{\"film\"}) }).reverse ;\n",
    "to-pretty-table($obj.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fe55e",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Complicated and neat workflow\n",
    "\n",
    "In this section we demonstrate a fairly complicated data wrangling sequence of operations that transforms [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe's_quartet) into a form that is easier to plot.\n",
    "\n",
    "**Remark:** Anscombe's quartet has four sets of points that have nearly the same x- and y- mean values. (But the sets have very different shapes.)\n",
    "\n",
    "### Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e17069e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$obj = dfAnscombe ;\n",
       "$obj = to-long-format( $obj ) ;\n",
       "$obj = separate-column( $obj, \"Variable\", (\"Variable\", \"Set\"), sep => \"\" ) ;\n",
       "$obj = to-wide-format( $obj, identifierColumns => (\"Set\", \"AutomaticKey\"), variablesFrom => \"Variable\", valuesFrom => \"Value\" )"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $command6 =\n",
    "        'use dfAnscombe;\n",
    "convert to long form;\n",
    "separate the data column Variable into Variable and Set with separator pattern \"\";\n",
    "to wide form for id columns Set and AutomaticKey variable column Variable and value column Value';\n",
    "\n",
    "ToDataQueryWorkflowCode($command6, target => $examplesTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae610f",
   "metadata": {},
   "source": [
    "### Execution steps\n",
    "\n",
    "Get a copy of the dataset into a \"pipeline object\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e60d5df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+----+----------+-----------+----+----+\n",
      "|     y1    |     y3    | x2 | x4 |    y2    |     y4    | x3 | x1 |\n",
      "+-----------+-----------+----+----+----------+-----------+----+----+\n",
      "|  8.040000 |  7.460000 | 10 | 8  | 9.140000 |  6.580000 | 10 | 10 |\n",
      "|  6.950000 |  6.770000 | 8  | 8  | 8.140000 |  5.760000 | 8  | 8  |\n",
      "|  7.580000 | 12.740000 | 13 | 8  | 8.740000 |  7.710000 | 13 | 13 |\n",
      "|  8.810000 |  7.110000 | 9  | 8  | 8.770000 |  8.840000 | 9  | 9  |\n",
      "|  8.330000 |  7.810000 | 11 | 8  | 9.260000 |  8.470000 | 11 | 11 |\n",
      "|  9.960000 |  8.840000 | 14 | 8  | 8.100000 |  7.040000 | 14 | 14 |\n",
      "|  7.240000 |  6.080000 | 6  | 8  | 6.130000 |  5.250000 | 6  | 6  |\n",
      "|  4.260000 |  5.390000 | 4  | 19 | 3.100000 | 12.500000 | 4  | 4  |\n",
      "| 10.840000 |  8.150000 | 12 | 8  | 9.130000 |  5.560000 | 12 | 12 |\n",
      "|  4.820000 |  6.420000 | 7  | 8  | 7.260000 |  7.910000 | 7  | 7  |\n",
      "|  5.680000 |  5.730000 | 5  | 8  | 4.740000 |  6.890000 | 5  | 5  |\n",
      "+-----------+-----------+----+----+----------+-----------+----+----+\n"
     ]
    }
   ],
   "source": [
    "my $obj = @dfAnscombe;\n",
    "say to-pretty-table($obj);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945f8c3",
   "metadata": {},
   "source": [
    "Summarize Anscombe's quartet (using \"Data::Summarizers\", [AAp3]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dfd8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------+--------------------+--------------------+--------------+--------------+--------------------+\n",
      "| y3              | x3           | x1           | y2                 | y1                 | x2           | x4           | y4                 |\n",
      "+-----------------+--------------+--------------+--------------------+--------------------+--------------+--------------+--------------------+\n",
      "| Min    => 5.39  | Min    => 4  | Min    => 4  | Min    => 3.1      | Min    => 4.26     | Min    => 4  | Min    => 8  | Min    => 5.25     |\n",
      "| 1st-Qu => 6.08  | 1st-Qu => 6  | 1st-Qu => 6  | 1st-Qu => 6.13     | 1st-Qu => 5.68     | 1st-Qu => 6  | 1st-Qu => 8  | 1st-Qu => 5.76     |\n",
      "| Mean   => 7.5   | Mean   => 9  | Mean   => 9  | Mean   => 7.500909 | Mean   => 7.500909 | Mean   => 9  | Mean   => 9  | Mean   => 7.500909 |\n",
      "| Median => 7.11  | Median => 9  | Median => 9  | Median => 8.14     | Median => 7.58     | Median => 9  | Median => 8  | Median => 7.04     |\n",
      "| 3rd-Qu => 8.15  | 3rd-Qu => 12 | 3rd-Qu => 12 | 3rd-Qu => 9.13     | 3rd-Qu => 8.81     | 3rd-Qu => 12 | 3rd-Qu => 8  | 3rd-Qu => 8.47     |\n",
      "| Max    => 12.74 | Max    => 14 | Max    => 14 | Max    => 9.26     | Max    => 10.84    | Max    => 14 | Max    => 19 | Max    => 12.5     |\n",
      "+-----------------+--------------+--------------+--------------------+--------------------+--------------+--------------+--------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({x1 => Min    => 4, x2 => Min    => 4, x3 => Min    => 4, x4 => Min    => 8, y1 => Min    => 4.26, y2 => Min    => 3.1, y3 => Min    => 5.39, y4 => Min    => 5.25} {x1 => 1st-Qu => 6, x2 => 1st-Qu => 6, x3 => 1st-Qu => 6, x4 => 1st-Qu => 8, y1 => 1st-Qu => 5.68, y2 => 1st-Qu => 6.13, y3 => 1st-Qu => 6.08, y4 => 1st-Qu => 5.76} {x1 => Mean   => 9, x2 => Mean   => 9, x3 => Mean   => 9, x4 => Mean   => 9, y1 => Mean   => 7.500909, y2 => Mean   => 7.500909, y3 => Mean   => 7.5, y4 => Mean   => 7.500909} {x1 => Median => 9, x2 => Median => 9, x3 => Median => 9, x4 => Median => 8, y1 => Median => 7.58, y2 => Median => 8.14, y3 => Median => 7.11, y4 => Median => 7.04} {x1 => 3rd-Qu => 12, x2 => 3rd-Qu => 12, x3 => 3rd-Qu => 12, x4 => 3rd-Qu => 8, y1 => 3rd-Qu => 8.81, y2 => 3rd-Qu => 9.13, y3 => 3rd-Qu => 8.15, y4 => 3rd-Qu => 8.47} {x1 => Max    => 14, x2 => Max    => 14, x3 => Max    => 14, x4 => Max    => 19, y1 => Max    => 10.84, y2 => Max    => 9.26, y3 => Max    => 12.74, y4 => Max    => 12.5})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records-summary($obj);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6f2c2",
   "metadata": {},
   "source": [
    "**Remark:** From the table above it is not clear how exactly we have to access the data in order \n",
    "to plot each of Anscombe's sets. The data wrangling steps below show a way to separate the sets\n",
    "and make them amenable for set-wise manipulations.\n",
    "\n",
    "Very often values of certain data parameters are conflated and put into dataset's column names.\n",
    "(As in Anscombe's dataset.)\n",
    "\n",
    "In those situations we:\n",
    "\n",
    "- Convert the dataset into long format, since that allows column names to be treated as data\n",
    "\n",
    "- Separate the values of a certain column into to two or more columns\n",
    "\n",
    "Reshape the \"pipeline object\" into\n",
    "[long format](https://en.wikipedia.org/wiki/Wide_and_narrow_data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64d3b6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+----------+--------------+\n",
       "| Variable |  Value   | AutomaticKey |\n",
       "+----------+----------+--------------+\n",
       "|    x2    |    10    |      0       |\n",
       "|    y3    | 7.460000 |      0       |\n",
       "|    x1    |    10    |      0       |\n",
       "|    y4    | 6.580000 |      0       |\n",
       "|    y1    | 8.040000 |      0       |\n",
       "|    x3    |    10    |      0       |\n",
       "|    y2    | 9.140000 |      0       |\n",
       "+----------+----------+--------------+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = to-long-format($obj);\n",
    "to-pretty-table($obj.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62e7a3",
   "metadata": {},
   "source": [
    "Separate the data column \"Variable\" into the columns \"Variable\" and \"Set\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70602dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------+----------+-----+\n",
       "| Variable | AutomaticKey |  Value   | Set |\n",
       "+----------+--------------+----------+-----+\n",
       "|    x     |      0       |    10    |  2  |\n",
       "|    y     |      0       | 7.460000 |  3  |\n",
       "|    x     |      0       |    10    |  1  |\n",
       "|    y     |      0       | 6.580000 |  4  |\n",
       "|    y     |      0       | 8.040000 |  1  |\n",
       "|    x     |      0       |    10    |  3  |\n",
       "|    y     |      0       | 9.140000 |  2  |\n",
       "+----------+--------------+----------+-----+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = separate-column( $obj, \"Variable\", (\"Variable\", \"Set\"), sep => \"\" ) ;\n",
    "to-pretty-table($obj.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82f6ac",
   "metadata": {},
   "source": [
    "Reshape the \"pipeline object\" into\n",
    "[wide format](https://en.wikipedia.org/wiki/Wide_and_narrow_data)\n",
    "using appropriate identifier-, variable-, and value column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60d561c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+------+-----+--------------+\n",
       "| x  |  y   | Set | AutomaticKey |\n",
       "+----+------+-----+--------------+\n",
       "| 10 | 8.04 |  1  |      0       |\n",
       "| 8  | 6.95 |  1  |      1       |\n",
       "| 13 | 7.58 |  1  |      2       |\n",
       "| 9  | 8.81 |  1  |      3       |\n",
       "| 11 | 8.33 |  1  |      4       |\n",
       "| 14 | 9.96 |  1  |      5       |\n",
       "| 6  | 7.24 |  1  |      6       |\n",
       "+----+------+-----+--------------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$obj = to-wide-format( $obj, identifierColumns => (\"Set\", \"AutomaticKey\"), variablesFrom => \"Variable\", valuesFrom => \"Value\" );\n",
    "to-pretty-table($obj.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1379c",
   "metadata": {},
   "source": [
    "Plot each dataset of Anscombe's quartet (using [\"Text::Plot\"](https://raku.land/zef:antononcube/Text::Plot), [AAp6]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0273e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                          Set : 3                           \n",
      "+---+---------+---------+----------+---------+---------+---+       \n",
      "|                                                          |       \n",
      "|                                                 *        |       \n",
      "+                                                          +  12.00\n",
      "|                                                          |       \n",
      "|                                                          |       \n",
      "+                                                          +  10.00\n",
      "|                                                      *   |       \n",
      "|                                            *             |       \n",
      "+                                  *    *                  +   8.00\n",
      "|                       *    *                             |       \n",
      "|             *    *                                       |       \n",
      "+   *    *                                                 +   6.00\n",
      "|                                                          |       \n",
      "+---+---------+---------+----------+---------+---------+---+       \n",
      "    4.00      6.00      8.00       10.00     12.00     14.00       \n",
      "\n",
      "                          Set : 2                           \n",
      "+---+---------+---------+----------+---------+---------+---+      \n",
      "|                                                          |      \n",
      "+                            *     *    *    *    *        +  9.00\n",
      "|                                                          |      \n",
      "+                       *                              *   +  8.00\n",
      "|                  *                                       |      \n",
      "+                                                          +  7.00\n",
      "+             *                                            +  6.00\n",
      "|                                                          |      \n",
      "+                                                          +  5.00\n",
      "|        *                                                 |      \n",
      "+                                                          +  4.00\n",
      "|   *                                                      |      \n",
      "+                                                          +  3.00\n",
      "+---+---------+---------+----------+---------+---------+---+      \n",
      "    4.00      6.00      8.00       10.00     12.00     14.00      \n",
      "\n",
      "                          Set : 4                           \n",
      "+---+--------+--------+---------+--------+---------+-------+       \n",
      "|                                                          |       \n",
      "+                                                      *   +  12.00\n",
      "|                                                          |       \n",
      "|                                                          |       \n",
      "+                                                          +  10.00\n",
      "|                                                          |       \n",
      "|   *                                                      |       \n",
      "+   *                                                      +   8.00\n",
      "|   *                                                      |       \n",
      "|   *                                                      |       \n",
      "+                                                          +   6.00\n",
      "|   *                                                      |       \n",
      "|                                                          |       \n",
      "+---+--------+--------+---------+--------+---------+-------+       \n",
      "    8.00     10.00    12.00     14.00    16.00     18.00           \n",
      "\n",
      "                          Set : 1                           \n",
      "+---+---------+---------+----------+---------+---------+---+       \n",
      "|                                                          |       \n",
      "|                                            *             |       \n",
      "+                                                      *   +  10.00\n",
      "|                                                          |       \n",
      "|                            *                             |       \n",
      "+                                  *    *                  +   8.00\n",
      "|                                                 *        |       \n",
      "|             *         *                                  |       \n",
      "|                                                          |       \n",
      "+        *                                                 +   6.00\n",
      "|                                                          |       \n",
      "|   *              *                                       |       \n",
      "+                                                          +   4.00\n",
      "+---+---------+---------+----------+---------+---------+---+       \n",
      "    4.00      6.00      8.00       10.00     12.00     14.00       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True True True True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group-by($obj, 'Set').map({ say \"\\n\", text-list-plot( $_.value.map({ +$_<x> }).List, $_.value.map({ +$_<y> }).List, title => 'Set : ' ~ $_.key) })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0f089",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## References\n",
    "\n",
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov,\n",
    "[\"Introduction to data wrangling with Raku\"](https://rakuforprediction.wordpress.com/2021/12/31/introduction-to-data-wrangling-with-raku/)\n",
    ",\n",
    "(2021), \n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[AA2] Anton Antonov,\n",
    "[\"Увод в обработката на данни с Raku\"](https://rakuforprediction.wordpress.com/2022/05/24/увод-в-обработката-на-данни-с-raku/)\n",
    ",\n",
    "(2022),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[AA3] Anton Antonov,\n",
    "[\"Raku Text::CodeProcessing\"](https://rakuforprediction.wordpress.com/2021/07/13/raku-textcodeprocessing/),\n",
    "(2021),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[HW1] Hadley Wickham,\n",
    "[\"The Split-Apply-Combine Strategy for Data Analysis\"](https://www.jstatsoft.org/article/view/v040i01),\n",
    "(2011),\n",
    "[Journal of Statistical Software](https://www.jstatsoft.org/).\n",
    "\n",
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[DSL::English::DataQueryWorkflows Raku package](https://github.com/antononcube/Raku-DSL-English-DataQueryWorkflows),\n",
    "(2020-2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[DSL::Bulgarian Raku package](https://github.com/antononcube/Raku-DSL-Bulgarian),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[DSL::Shared::Utilities::ComprehensiveTranslations Raku package](https://github.com/antononcube/Raku-Text-Plot),\n",
    "(2020-2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[Data::Generators Raku package](https://github.com/antononcube/Raku-Data-Generators),\n",
    "(2021),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[Data::Reshapers Raku package](https://github.com/antononcube/Raku-Data-Reshapers),\n",
    "(2021),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[Data::Summarizers Raku package](https://github.com/antononcube/Raku-Data-Summarizers),\n",
    "(2021),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[Text::CodeProcessing Raku package](https://github.com/antononcube/Raku-Text-CodeProcessing),\n",
    "(2021),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[Text::Plot Raku package](https://github.com/antononcube/Raku-Text-Plot),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167894e",
   "metadata": {},
   "source": [
    "### Videos\n",
    "\n",
    "[AAv1] Anton Antonov,\n",
    "[\"Multi-language Data-Wrangling Conversational Agent\"](https://www.youtube.com/watch?v=pQk5jwoMSxs),\n",
    "(2020),\n",
    "[Wolfram Technology Conference 2020, YouTube/Wolfram](https://www.youtube.com/channel/UCJekgf6k62CQHdENWf2NgAQ).\n",
    "\n",
    "[AAv2] Anton Antonov,\n",
    "[\"Implementation of ML algorithms in Raku\"](https://www.youtube.com/watch?v=efRHfjYebs4),\n",
    "(2022),\n",
    "[Anton A. Antonov's channel at YouTube](https://www.youtube.com/channel/UC5qMPIsJeztfARXWdIw3Xzw).\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Raku",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/plain",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
