# Graph representation of grammars

## Introduction

This computational Markdown document demonstrates the mapping of 
[Extended Backus-Naur Grammars (EBNF)](https://en.wikipedia.org/wiki/Extended_Backus–Naur_form)
and Raku grammars into graphs.

The graphs are made with [Mermaid-JS](https://mermaid.js.org) and 
[Wolfram Language (WL)](https://www.wolfram.com/language/) (aka Mathematica).  

**Remark:** Mermaid-JS specs are automatically rendered in GitHub Markdown files,
and have plug-ing support in Integrated Development Environments (IDEs) like IntelliJ IDEA, VS Code, Emacs, etc.

Examples using Large Language Models (LLMs) are provided. (Via 
["WWW::OpenAI"](https://raku.land/zef:antononcube/WWW::OpenAI) and 
["WWW::PaLM"](https://raku.land/zef:antononcube/WWW::PaLM), [AAp4, AAp5].)

The document has the following structure:

- **Reflections and observations**     
  I.e. "conclusions first." Those make the anecdotal examples below more scientific, not just conjecture-worthy anecdotes.
- **Packages and interactions**   
  A list the packages utilized and how they can be used in a coherent way.
- **Generating Mermaid diagrams for EBNFs**   
  Two simple and similar EBNF grammars with graphs for instructive comparison.
- **Generating graphs with LLMs**   
  Can LLMs replace Raku programming (of grammar-graphs making)?
- **Generating Mermaid diagrams for Raku grammars**   
  Same exercise but over *Raku grammars*, not some EBNFs... 
- **LLMs grammars for sentence collections**   
  An interesting way to derive EBNFs is to "just ask." Well, some questions might get really long (and their answers really expensive.) 
- **Random LLMs grammars**   
  Why ask for EBNFs of sentence collections, if we can just say "Give me a random EBNF grammar (or five.)"
- **More complicated grammar**   
  A larger grammar example, in order to illustrate the usefulness -- or *uselessness* -- of grammar-graphs.

------

## Reflections and observations

- I consider grammar graph representation "neat" and "cool" for small enough grammars, 
but I am not sure how useful it is for large grammars. 
  - Especially, large grammars with recursive dependencies between the production rules.

- I made a fair amount of experiments with relatively small grammars, and fewer experiments with a few large grammars.

- The ability to make the grammar graphs, of course, has at least didactic utility.

- Another utility of the examples given below is to show coherent interaction between the packages:
  - ["FunctionalParsers"](https://raku.land/zef:antononcube/FunctionalParsers)
  - ["EBNF::Grammar"](https://raku.land/zef:antononcube/EBNF::Grammar)
  - ["Grammar::TokenProcessing"](https://raku.land/zef:antononcube/Grammar-TokenProcessing)

- This Markdown document is "executed" with the package 
["Text::CodeProcessing"](https://raku.land/zef:antononcube/Text::CodeProcessing),
which allows generation of Markdown specs that are, say, automatically processed by Web browsers, IDEs, etc.  
  - Like Mermaid-JS charts and graphs.

- Visualizing grammars generated by Large Language Models (LLMs) -- 
like, [ChatGPT](https://openai.com/blog/chatgpt) and [PaLM](https://en.wikipedia.org/wiki/PaLM) 
-- is both didactic and "neat." 
  - One of my primary motivations for making the packages "FunctionalParsers" and "EBNF::Grammar" was to be able 
  to easily (automatically or semi-automatically) process grammars generated with LLMs. 
  - It is not trivial to parse EBNF hallucinations by LLMs. (More details below.)

- This Markdown document can be converted into a Mathematica notebook using "Markdown::Grammar", [AAp6].
Mathematica notebooks in [RakuMode](https://resources.wolframcloud.com/PacletRepository/resources/AntonAntonov/RakuMode/), 
[AAp7], make much easier the experiments with diagram generation and LLM utilization. (And more fun!) 

- Generating Raku grammars with ChatGPT-3.5 or PaLM often produces "non-working" grammars. Hence, I focused on EBNF grammars.
  My assumption is that EBNF has been around for a longer time period, hence, LLMs are "better trained for it."

------

## Packages and interactions

Here we load the packages used below:

```perl6
use FunctionalParsers;
use FunctionalParsers::EBNF;
use EBNF::Grammar;
use Grammar::TokenProcessing;
use WWW::OpenAI;
use WWW::PaLM;
```
```
# (Any)
```

Here are flowcharts that summarize use cases, execution paths, and interaction between the packages:

```mermaid
graph LR
    FPs[["FunctionalParsers"]]
    grEBNF>EBNF<br>grammar]
    mmdGraph>Mermaid JS<br>graph spec]
    grEBNF --> |fp-grammar-graph|FPs --> mmdGraph
```

```mermaid
graph LR
    FPs[["FunctionalParsers"]]
    EBNFGram[["EBNF::Grammar"]]
    grEBNF>EBNF<br>grammar]
    mmdGraph>Mermaid JS<br>graph spec]
    grEBNF --> |parse|EBNFGram --> |fp-grammar-graph|FPs --> mmdGraph
```

```mermaid
graph LR
    FPs[["FunctionalParsers"]]
    GramTP[["Grammar::TokenProcessing"]]
    grEBNF>EBNF<br>grammar]
    grRaku>Raku<br>grammar]
    mmdGraph>Mermaid JS<br>graph spec]
    grRaku --> |translate|GramTP --> grEBNF --> |fp-grammar-graph|FPs --> mmdGraph
```

```mermaid
graph LR
    FPs[["FunctionalParsers"]]
    GramTP[["Grammar::TokenProcessing"]]
    EBNFGram[["EBNF::Grammar"]]
    grEBNF>EBNF<br>grammar]
    RandSents>Random sentences]
    ChatGPT{{ChatGPT}}
    PaLM{{PaLM}}
    WWWOpenAI[[WWW::OpenAI]]
    WWWPaLM[[WWW::PaLM]]
    UInput[/Sentece<br>collection/]
    UInput --> WWWOpenAI
    UInput --> WWWPaLM
    ChatGPT -.- WWWOpenAI --> grEBNF
    PaLM -.- WWWPaLM --> grEBNF
    grEBNF --> |normalize|EBNFGram --> |fp-random-sentence|FPs --> RandSents 
    grEBNF --> |parse|EBNFGram --> |random-sentence-generation|GramTP --> RandSents 
```

------

## Generating Mermaid diagrams for EBNFs

The function `fp-ebnf-parse` can produce
[Mermaid-JS diagrams](https://mermaid.js.org)
corresponding to grammars with the target "MermaidJS::Graph".
Here is an example:

```perl6, output-lang=mermaid, output-prompt=NONE
my $ebnfCode1 = q:to/END/;
<top> = <a> | <b> ;
<a> = 'a' , { 'A' } , [ '1' ];
<b> = 'b' , ( 'B' | '2' );
END

fp-ebnf-parse($ebnfCode1, target=>'MermaidJS::Graph', dir-spec => 'LR').head.tail
```
```mermaid
graph LR
	NT:top["top"]
	alt14((or))
	rep7((*))
	seq12((and))
	T:2("2")
	opt9((?))
	T:A("A")
	NT:b["b"]
	T:a("a")
	T:b("b")
	NT:a["a"]
	T:1("1")
	alt1((or))
	seq5((and))
	T:B("B")
	alt1 --> NT:a
	alt1 --> NT:b
	NT:top --> alt1
	rep7 --> T:A
	opt9 --> T:1
	seq5 --> |1|T:a
	seq5 --> |2|rep7
	seq5 --> |3|opt9
	NT:a --> seq5
	alt14 --> T:B
	alt14 --> T:2
	seq12 --> |1|T:b
	seq12 --> |2|alt14
	NT:b --> seq12
```

Here is a legend:

- The non-terminals are shown with rectangles
- The terminals are shown with round rectangles
- The "conjunctions" are shown in disks
- The order of parsing in sequences is indicated with integer labels
- Pick-left and pick-right sequences use the labels "L" and "R" for the corresponding branches 

**Remark:** The Markdown cell above has the parameters `output-lang=mermaid, output-prompt=NONE`
which allow for direct diagram rendering of the obtained Mermaid code in various Markdown viewers (GitHub, IntelliJ, etc.)

Compare the following EBNF grammar and corresponding diagram with the ones above:

```perl6, output-lang=mermaid, output-prompt=NONE
my $ebnfCode2 = q:to/END/;
<top> = <a> | <b> ;
<a> = 'a' <& { 'A' } , [ '1' ] ;
<b> = 'b' , 'B' | '2' ;
END

fp-ebnf-parse($ebnfCode2, target=>'MermaidJS::Graph', dir-spec => 'LR').head.tail
```
```mermaid
graph LR
	T:a("a")
	T:B("B")
	T:b("b")
	opt10((?))
	rep8((*))
	T:1("1")
	alt1((or))
	seq7((and))
	T:2("2")
	alt13((or))
	NT:top["top"]
	T:A("A")
	seq14((and))
	seqL5(("«and"))
	NT:a["a"]
	NT:b["b"]
	alt1 --> NT:a
	alt1 --> NT:b
	NT:top --> alt1
	rep8 --> T:A
	opt10 --> T:1
	seq7 --> |1|rep8
	seq7 --> |2|opt10
	seqL5 --> |L|T:a
	seqL5 -.-> |R|seq7
	NT:a --> seqL5
	seq14 --> |1|T:b
	seq14 --> |2|T:B
	alt13 --> seq14
	alt13 --> T:2
	NT:b --> alt13
```

------

## Generating graphs with LLMs

It is interesting to see do LLMs do better at producing (Mermaid-JS) graph representations.

More importantly, we want to answer the question:

> Can we generate graph-specs (like, Mermaid-JS) without the need of programming the corresponding interpreters?

Here is a LLM request for a Mermaid-JS spec generation for one of the simple grammars above:

```perl6
my $request = "Make a Mermaid JS diagram for the EBNF grammar:\n$ebnfCode1";
#my $mmdLLM = openai-completion($request, max-tokens => 600, format => 'values', temperature => 1.2);
my $mmdLLM = palm-generate-text($request, max-tokens => 600, format => 'values', temperature => 0.9);
```
```
# ```mermaid
# graph LR
# top[top]
# a[a] --> top
# b[b] --> top
# a --> "a"
# a --> "{"
# a --> "A"
# a --> "}"
# a --> "["
# a --> "1"
# a --> "]"
# b --> "b"
# b --> "("
# b --> "B"
# b --> ")"
# b --> "2"
# ```
```

Here is the corresponding graph:

```perl6, results=asis
$mmdLLM
```
```mermaid
graph LR
top[top]
a[a] --> top
b[b] --> top
a --> "a"
a --> "{"
a --> "A"
a --> "}"
a --> "["
a --> "1"
a --> "]"
b --> "b"
b --> "("
b --> "B"
b --> ")"
b --> "2"
```


**Remark::** After multiple experiments I can say that the obtained Mermaid-JS code is either:
- Simple, somewhat relevant, and wrong
- Closer to correct after suitable manual editing 

As for the question above -- the answer is "No". But the LLM answers provide (somewhat) good initial versions
for manual (human) building of graph specifications. 

------

## Generating Mermaid diagrams for Raku grammars

In order to generate graphs for Raku grammars we use the following steps:

1. Translate Raku-grammar code into EBNF code
2. Translate EBNF code into graph code (Mermaid-JS or WL)

Consider a grammar for parsing proclaimed feeling toward different programming languages:

```perl6
grammar LangLove {
    rule TOP  { <workflow-command> }
    rule workflow-command  { <who> 'really'? <love> <lang> }
    token who { 'I' | 'We' }
    token love { 'hate' | 'love' }
    token lang { 'Raku' | 'Perl' | 'Rust' | 'Go' | 'Python' | 'Ruby' }
}
```
```
# (LangLove)
```

Here is an example parsing:

```perl6
LangLove.parse('I hate Perl')
```
```
# ｢I hate Perl｣
#  workflow-command => ｢I hate Perl｣
#   who => ｢I｣
#   love => ｢hate｣
#   lang => ｢Perl｣
```

First we derive the corresponding EBNF grammar:

```perl6
my $ebnfLangLove = to-ebnf-grammar(LangLove)
```
```
# <TOP> = <workflow-command>  ;
# <lang> = 'Raku'  | 'Perl'  | 'Rust'  | 'Go'  | 'Python'  | 'Ruby'  ;
# <love> = 'hate'  | 'love'  ;
# <who> = 'I'  | 'We'  ;
# <workflow-command> = <who> , ['really'] , <love> , <lang>  ;
```

Here is the corresponding Mermaid-JS graph:

```perl6, output.lang=mermaid, output.prompt=NONE
fp-grammar-graph($ebnfLangLove)
```
```mermaid
graph TD
	opt21((?))
	NT:who["who"]
	T:Go("Go")
	alt11((or))
	NT:love["love"]
	NT:lang["lang"]
	alt3((or))
	NT:workflow-command["workflow-command"]
	T:Ruby("Ruby")
	T:really("really")
	T:Rust("Rust")
	alt15((or))
	T:Raku("Raku")
	T:love("love")
	T:Perl("Perl")
	T:hate("hate")
	T:I("I")
	NT:TOP["TOP"]
	T:We("We")
	seq19((and))
	T:Python("Python")
	NT:TOP --> NT:workflow-command
	alt3 --> T:Raku
	alt3 --> T:Perl
	alt3 --> T:Rust
	alt3 --> T:Go
	alt3 --> T:Python
	alt3 --> T:Ruby
	NT:lang --> alt3
	alt11 --> T:hate
	alt11 --> T:love
	NT:love --> alt11
	alt15 --> T:I
	alt15 --> T:We
	NT:who --> alt15
	opt21 --> T:really
	seq19 --> |1|NT:who
	seq19 --> |2|opt21
	seq19 --> |3|NT:love
	seq19 --> |4|NT:lang
	NT:workflow-command --> seq19
```

------

## LLMs grammars for sentence collections

Here is an EBNF grammar generated with ChatGPT, [AAp4], over a list of chemical formulas:

```perl6
#my @sentences = <BrI BrClH2Si CCl4 CH3I C2H5Br H2O H2O4S AgBr AgBrO AgBrO2 AgBrO3 AgBrO4 AgCL>;
my @sentences = <AgBr AgBrO AgBrO2 AgBrO3 AgBrO4 AgCL>;
my $request = "Generate EBNF grammar for the sentences:\n{@sentences.map({ $_.comb.join(' ')}).join("\n")}";
#my $ebnfLLM = openai-completion($request, max-tokens => 600, format => 'values');
my $ebnfLLM = palm-generate-text($request, max-tokens => 600, format => 'values');
```
```
# ```
# <sentence> ::= <element> <element>
# <element> ::= <letter> | <letter> <element>
# <letter> ::= A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z
# ```
```

Often LLM requests as the ones above return code as Markdown code cells, hence, we try to remove the code cell markings:

```perl6
$ebnfLLM = $ebnfLLM.subst(/ ^ '`' ** 3 <-[\v]>* \n | '`' ** 3 \h* $ /,''):g;
```
```
# <sentence> ::= <element> <element>
# <element> ::= <letter> | <letter> <element>
# <letter> ::= A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z
```

```perl6, output.lang=mermaid, output.prompt=NONE
fp-grammar-graph($ebnfLLM, style => Whatever)
```
```mermaid
graph TD
	alt5((or))
	NT:H["H"]
	NT:F["F"]
	NT:sentence["sentence"]
	alt11((or))
	NT:U["U"]
	NT:K["K"]
	NT:M["M"]
	NT:Q["Q"]
	NT:E["E"]
	NT:V["V"]
	NT:I["I"]
	NT:W["W"]
	seq1((and))
	NT:B["B"]
	NT:X["X"]
	NT:Z["Z"]
	NT:L["L"]
	NT:S["S"]
	NT:R["R"]
	NT:P["P"]
	NT:O["O"]
	NT:T["T"]
	NT:letter["letter"]
	NT:N["N"]
	NT:Y["Y"]
	seq7((and))
	NT:D["D"]
	NT:G["G"]
	NT:C["C"]
	NT:J["J"]
	NT:A["A"]
	NT:element["element"]
	seq1 --> |1|NT:element
	seq1 --> |2|NT:element
	NT:sentence --> seq1
	seq7 --> |1|NT:letter
	seq7 --> |2|NT:element
	alt5 --> NT:letter
	alt5 --> seq7
	NT:element --> alt5
	alt11 --> NT:A
	alt11 --> NT:B
	alt11 --> NT:C
	alt11 --> NT:D
	alt11 --> NT:E
	alt11 --> NT:F
	alt11 --> NT:G
	alt11 --> NT:H
	alt11 --> NT:I
	alt11 --> NT:J
	alt11 --> NT:K
	alt11 --> NT:L
	alt11 --> NT:M
	alt11 --> NT:N
	alt11 --> NT:O
	alt11 --> NT:P
	alt11 --> NT:Q
	alt11 --> NT:R
	alt11 --> NT:S
	alt11 --> NT:T
	alt11 --> NT:U
	alt11 --> NT:V
	alt11 --> NT:W
	alt11 --> NT:X
	alt11 --> NT:Y
	alt11 --> NT:Z
	NT:letter --> alt11
```

Another way for "verify" a grammar is to generate random sentences with it:

```perl6
.say for ebnf-random-sentence($ebnfLLM, 12, style => Whatever)
```
```
# O 
# A 
# G H  
# I 
# D N  
# I L 
# J L 
# P Y 
# J F  
# M I 
# K 
# P J
```

**Remark:** Random sentences can be also generated with the function `fp-random-sentence` provided by "FunctionalParsers".

**Remark:** The function `ebnf-random-sentence` uses `fp-random-sentence`, but `ebnf-random-sentence` 
(parses and) standardizes the given EBNF grammar first, (then it gives the standardized grammar to `fp-random-sentence`.)

**Remark:** It is not trivial to parse EBNF hallucinations by LLMs. For the same EBNF-making request a given LLM 
can produce different EBNF grammars, each having "its own" EBNF style. Hence, both "FunctionalParsers" and "EBNF::Grammar"
have parsers for different EBNF styles. With the spec `style => Whatever` parsing of all of the "anticipated" styles are attempted.

------

## Random LLMs grammars

Why ask for EBNF graphs with sentence collections, if we can just say:

> Give me a random EBNF grammar. (Or five.) 
 
**Remark:** Note, the implications of testing the parsers in that way. We can try to produce extensive parser tests
by multiple randomly obtained grammars from different LLMs. (Using different LLM parameters, like, temperatures, etc.)

Here is another example using a ***random*** (hopefully small) EBNF grammar:

```perl6
my $request2 = "Give an example of simple EBNF grammar.";
#my $ebnfLLM2 = openai-completion($request2, max-tokens => 600, format => 'values', temperature => 1.2);
my $ebnfLLM2 = palm-generate-text($request2, max-tokens => 600, format => 'values', temperature => 0.9);
$ebnfLLM2 = $ebnfLLM2.subst(/ ^ '`' ** 3 <-[\v]>* \n | '`' ** 3 \h* $ /,''):g;
```
```
# <expression> ::= <term> { <addop> <term> }
# <term> ::= <factor> { <mulop> <factor> }
# <factor> ::= <integer> | <float> | <variable> | <parenthesis>
# <addop> ::= + | -
# <mulop> ::= * | /
# <parenthesis> ::= ( <expression> )
```

```perl6, output.lang=mermaid, output.prompt=NONE
fp-grammar-graph($ebnfLLM2, style => Whatever, dir-spec => 'LR')
```
```mermaid
graph LR
	NT:factor["factor"]
	seq4((and))
	seq1((and))
	NT:float["float"]
	NT:parenthesis["parenthesis"]
	NT:integer["integer"]
	NT:term["term"]
	NT:addop["addop"]
	rep10((*))
	NT:variable["variable"]
	NT:mulop["mulop"]
	rep3((*))
	alt15((or))
	seq11((and))
	NT:expression["expression"]
	seq8((and))
	seq4 --> |1|NT:addop
	seq4 --> |2|NT:term
	rep3 --> seq4
	seq1 --> |1|NT:term
	seq1 --> |2|rep3
	NT:expression --> seq1
	seq11 --> |1|NT:mulop
	seq11 --> |2|NT:factor
	rep10 --> seq11
	seq8 --> |1|NT:factor
	seq8 --> |2|rep10
	NT:term --> seq8
	alt15 --> NT:integer
	alt15 --> NT:float
	alt15 --> NT:variable
	alt15 --> NT:parenthesis
	NT:factor --> alt15
```

------

## More complicated grammar

Consider the following grammar for arithmetic expressions:

```perl6
my $ebnfExpr = q:to/END/;
start   = expr ;
expr    = term '+' expr | term '-' expr | term ;
term    = term '*' factor | term '/' factor | factor ;
factor  = '+' factor | '-' factor | (expr) | integer | integer '.' integer ;
integer = digit integer | digit ;
digit   = '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' ;
END
```
```
# start   = expr ;
# expr    = term '+' expr | term '-' expr | term ;
# term    = term '*' factor | term '/' factor | factor ;
# factor  = '+' factor | '-' factor | (expr) | integer | integer '.' integer ;
# integer = digit integer | digit ;
# digit   = '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' ;
```

Here we produce the graph using special parsing style:

```perl6, output.lang=mermaid, output.prompt=NONE
fp-grammar-graph($ebnfExpr, style => 'Relaxed')
```
```mermaid
graph TD
	NT:expr["expr"]
	seq29((and))
	T:8("8")
	seq19((and))
	T:HYPHEN-MINUS("-")
	T:6("6")
	seq40((and))
	alt25((or))
	T:FULL_STOP(".")
	alt3((or))
	T:9("9")
	seq34((and))
	T:PLUS_SIGN("+")
	seq8((and))
	seq26((and))
	T:SOLIDUS("/")
	T:7("7")
	alt39((or))
	NT:factor["factor"]
	alt14((or))
	T:5("5")
	T:3("3")
	seq15((and))
	T:1("1")
	seq4((and))
	T:2("2")
	T:0("0")
	NT:integer["integer"]
	NT:term["term"]
	alt45((or))
	NT:start["start"]
	T:ASTERISK("*")
	NT:digit["digit"]
	T:4("4")
	NT:start --> NT:expr
	seq4 --> |1|NT:term
	seq4 --> |2|T:PLUS_SIGN
	seq4 --> |3|NT:expr
	seq8 --> |1|NT:term
	seq8 --> |2|T:HYPHEN-MINUS
	seq8 --> |3|NT:expr
	alt3 --> seq4
	alt3 --> seq8
	alt3 --> NT:term
	NT:expr --> alt3
	seq15 --> |1|NT:term
	seq15 --> |2|T:ASTERISK
	seq15 --> |3|NT:factor
	seq19 --> |1|NT:term
	seq19 --> |2|T:SOLIDUS
	seq19 --> |3|NT:factor
	alt14 --> seq15
	alt14 --> seq19
	alt14 --> NT:factor
	NT:term --> alt14
	seq26 --> |1|T:PLUS_SIGN
	seq26 --> |2|NT:factor
	seq29 --> |1|T:HYPHEN-MINUS
	seq29 --> |2|NT:factor
	seq34 --> |1|NT:integer
	seq34 --> |2|T:FULL_STOP
	seq34 --> |3|NT:integer
	alt25 --> seq26
	alt25 --> seq29
	alt25 --> NT:expr
	alt25 --> NT:integer
	alt25 --> seq34
	NT:factor --> alt25
	seq40 --> |1|NT:digit
	seq40 --> |2|NT:integer
	alt39 --> seq40
	alt39 --> NT:digit
	NT:integer --> alt39
	alt45 --> T:0
	alt45 --> T:1
	alt45 --> T:2
	alt45 --> T:3
	alt45 --> T:4
	alt45 --> T:5
	alt45 --> T:6
	alt45 --> T:7
	alt45 --> T:8
	alt45 --> T:9
	NT:digit --> alt45
```

------

## References

### Articles

[Wk1] Wikipedia entry, ["Extended Backus–Naur form"](https://en.wikipedia.org/wiki/Extended_Backus–Naur_form).

### Packages, repositories

[AAp1] Anton Antonov,
[FunctionParsers Raku package](https://github.com/antononcube/Raku-FunctionalParsers),
(2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp2] Anton Antonov,
[EBNF:Grammar Raku package](https://github.com/antononcube/Raku-EBNF-Grammar),
(2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp3] Anton Antonov,
[Grammar::TokenProcessing Raku package](https://github.com/antononcube/Raku-Grammar-TokenProcessing),
(2022-2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp4] Anton Antonov,
[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),
(2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp5] Anton Antonov,
[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),
(2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp6] Anton Antonov,
[Markdown::Grammar Raku package](https://github.com/antononcube/Raku-Markdown-Grammar),
(2022-2023),
[GitHub/antononcube](https://github.com/antononcube).

[AAp7] Anton Antonov,
[RakuMode WL paclet](https://resources.wolframcloud.com/PacletRepository/resources/AntonAntonov/RakuMode/),
(2023),
[Wolfram Resource System / AntonAntonov](https://resources.wolframcloud.com/publishers/resources?PublisherID=AntonAntonov).
